{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5f0ffe",
   "metadata": {},
   "source": [
    "\n",
    "# MovieLens Baseline (PySpark ALS) – .ipynb\n",
    "\n",
    "This notebook is a drop-in PySpark alternative to a Surprise SVD baseline.  \n",
    "It loads MovieLens ratings, trains an **explicit-feedback ALS** model with simple hyperparameter tuning, evaluates **RMSE**, and exports **Top‑N recommendations** and a saved model.\n",
    "\n",
    "**Sections**\n",
    "1. Environment & Spark Bootstrap\n",
    "2. Load Ratings\n",
    "3. Basic Stats\n",
    "4. Train/Validation Split\n",
    "5. Train/Tune ALS (TrainValidationSplit)\n",
    "6. Evaluate RMSE\n",
    "7. Top‑N Recommendations\n",
    "8. Save/Load Model\n",
    "9. (Optional) Write validation predictions snapshot\n",
    "\n",
    "> Tested with: Python 3.10, PySpark 3.5.x, OpenJDK 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8fb16",
   "metadata": {},
   "source": [
    "## 1) Environment & Spark Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219ce58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/28 23:24:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MovieLens-ALS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x110337580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def build_spark(app_name: str = \"MovieLens-ALS\"):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "spark = build_spark()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d68099",
   "metadata": {},
   "source": [
    "## 2) Load Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f2e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ratings from: /Users/alanyu/Documents/IIT/ITM/ITMD-524-Applied AI and Deep Learning/finalproject/MovieLens-MCRS/data/raw/ratings.csv\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|1     |1      |4.0   |NULL     |\n",
      "|1     |3      |4.0   |NULL     |\n",
      "|1     |6      |4.0   |NULL     |\n",
      "|1     |47     |5.0   |NULL     |\n",
      "|1     |50     |5.0   |NULL     |\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "\n",
    "if not RAW_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected raw data at {RAW_DIR}. Run notebook from repo root or place ratings.csv there.\")\n",
    "\n",
    "def load_ratings(spark, ratings_path, fmt: str = 'csv', sep: str = ',', header: bool = True):\n",
    "    \"\"\"Load MovieLens ratings with schema.\"\"\"\n",
    "    path = Path(ratings_path).expanduser()\n",
    "    schema = T.StructType([\n",
    "        T.StructField('userId', T.IntegerType(), False),\n",
    "        T.StructField('movieId', T.IntegerType(), False),\n",
    "        T.StructField('rating', T.FloatType(), False),\n",
    "        T.StructField('timestamp', T.TimestampType(), True),\n",
    "    ])\n",
    "    reader = spark.read.schema(schema)\n",
    "    if fmt.lower() == 'csv':\n",
    "        reader = (\n",
    "            reader\n",
    "            .option('header', str(header).lower())\n",
    "            .option('sep', sep)\n",
    "            .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss')\n",
    "        )\n",
    "        df = reader.csv(str(path))\n",
    "    elif fmt.lower() in {'parquet', 'pq'}:\n",
    "        df = reader.parquet(str(path))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported fmt. Use 'csv' or 'parquet'.\")\n",
    "    df = (\n",
    "        df.dropna(subset=['userId', 'movieId', 'rating'])\n",
    "          .filter((F.col('rating') >= 0.5) & (F.col('rating') <= 5.0))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def resolve_ratings_path():\n",
    "    env_path = os.environ.get('MOVIELENS_RATINGS_PATH')\n",
    "    if env_path:\n",
    "        candidate = Path(env_path).expanduser()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        raise FileNotFoundError(f\"MOVIELENS_RATINGS_PATH={env_path!r} does not exist.\")\n",
    "    candidate = RAW_DIR / 'ratings.csv'\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    candidate = DATA_DIR / 'ratings.csv'\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find ratings.csv under {RAW_DIR}. Set MOVIELENS_RATINGS_PATH if stored elsewhere.\"\n",
    "    )\n",
    "\n",
    "RATINGS_PATH = resolve_ratings_path()\n",
    "FMT = 'csv'  # or 'parquet'\n",
    "ratings = load_ratings(spark, RATINGS_PATH, fmt=FMT, sep=',', header=True)\n",
    "print(f\"Loaded ratings from: {RATINGS_PATH}\")\n",
    "ratings.printSchema()\n",
    "ratings.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fb3e0",
   "metadata": {},
   "source": [
    "## 3) Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57e1351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Stats =====\n",
      "#Ratings: 100,836\n",
      "#Users:   610\n",
      "#Items:   9,724\n",
      "Density:  0.01699968\n",
      "+-----------------+\n",
      "|global_mean      |\n",
      "+-----------------+\n",
      "|3.501556983616962|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def basic_stats(ratings):\n",
    "    n_ratings = ratings.count()\n",
    "    n_users = ratings.select(\"userId\").distinct().count()\n",
    "    n_items = ratings.select(\"movieId\").distinct().count()\n",
    "    density = n_ratings / (n_users * n_items)\n",
    "    print(\"===== Dataset Stats =====\")\n",
    "    print(f\"#Ratings: {n_ratings:,}\")\n",
    "    print(f\"#Users:   {n_users:,}\")\n",
    "    print(f\"#Items:   {n_items:,}\")\n",
    "    print(f\"Density:  {density:.8f}\")\n",
    "    ratings.groupBy().agg(F.mean(\"rating\").alias(\"global_mean\")).show(truncate=False)\n",
    "\n",
    "basic_stats(ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d616ac0",
   "metadata": {},
   "source": [
    "## 4) Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5884dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 80,578, Val: 20,258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_train_val(ratings, seed: int = 42):\n",
    "    # For time-aware split, sort per user by timestamp and split; here we use random split.\n",
    "    train, val = ratings.randomSplit([0.8, 0.2], seed=seed)\n",
    "    return train.cache(), val.cache()\n",
    "\n",
    "train, val = split_train_val(ratings)\n",
    "print(f\"Train: {train.count():,}, Val: {val.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90c232",
   "metadata": {},
   "source": [
    "## 5) Train/Tune ALS (TrainValidationSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a34893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 23:25:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/10/28 23:25:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainValidationSplitModel_dc8d6a98d7a3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "def tune_and_fit(train, implicit: bool = False):\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        implicitPrefs=implicit,\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True,\n",
    "        seed=42,\n",
    "    )\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(als.rank, [16, 32, 64])\n",
    "        .addGrid(als.regParam, [0.05, 0.1, 0.2])\n",
    "        .addGrid(als.maxIter, [10, 15])\n",
    "        .build()\n",
    "    )\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    tvs = TrainValidationSplit(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        trainRatio=0.8,\n",
    "        parallelism=2,\n",
    "        seed=42,\n",
    "    )\n",
    "    return tvs.fit(train)\n",
    "\n",
    "tvs_model = tune_and_fit(train, implicit=False)\n",
    "tvs_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b82d3f",
   "metadata": {},
   "source": [
    "## 6) Evaluate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73b459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Best ALS Params =====\n",
      "rank=64, regParam=0.2, maxIter=15, implicitPrefs=False\n",
      "Validation RMSE = 0.8749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8748588135300418"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate(model, val):\n",
    "    best_model = model.bestModel\n",
    "    preds = best_model.transform(val)\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(preds)\n",
    "    print(\"===== Best ALS Params =====\")\n",
    "    print(f\"rank={best_model._java_obj.parent().getRank()}, \"\n",
    "          f\"regParam={best_model._java_obj.parent().getRegParam()}, \"\n",
    "          f\"maxIter={best_model._java_obj.parent().getMaxIter()}, \"\n",
    "          f\"implicitPrefs={best_model._java_obj.parent().getImplicitPrefs()}\")\n",
    "    print(f\"Validation RMSE = {rmse:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "rmse = evaluate(tvs_model, val)\n",
    "rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67c57e",
   "metadata": {},
   "source": [
    "## 7) Top‑N Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07e4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+\n",
      "|userId|movieId|score    |\n",
      "+------+-------+---------+\n",
      "|1     |132333 |5.928354 |\n",
      "|1     |96004  |5.9241643|\n",
      "|1     |3379   |5.9241643|\n",
      "|1     |33649  |5.6812305|\n",
      "|1     |60943  |5.6718407|\n",
      "|1     |59018  |5.6718407|\n",
      "|1     |5915   |5.556229 |\n",
      "|1     |102217 |5.548975 |\n",
      "|1     |93008  |5.5297456|\n",
      "|1     |77846  |5.5297456|\n",
      "|2     |67618  |4.811521 |\n",
      "|2     |96004  |4.767877 |\n",
      "|2     |3379   |4.767877 |\n",
      "|2     |131724 |4.741022 |\n",
      "|2     |33649  |4.71986  |\n",
      "|2     |184245 |4.6064463|\n",
      "|2     |134796 |4.6064463|\n",
      "|2     |117531 |4.6064463|\n",
      "|2     |86237  |4.6064463|\n",
      "|2     |84273  |4.6064463|\n",
      "|3     |6835   |4.77144  |\n",
      "|3     |5746   |4.77144  |\n",
      "|3     |5181   |4.6396904|\n",
      "|3     |4518   |4.482605 |\n",
      "|3     |2851   |4.3266068|\n",
      "|3     |7899   |4.294297 |\n",
      "|3     |26409  |4.074115 |\n",
      "|3     |3024   |3.6480796|\n",
      "|3     |3703   |3.3276978|\n",
      "|3     |4821   |3.2661479|\n",
      "+------+-------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2587:============================================>       (86 + 11) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|movieId|userId|score    |\n",
      "+-------+------+---------+\n",
      "|1      |53    |5.1805596|\n",
      "|1      |276   |4.7790174|\n",
      "|1      |43    |4.7342415|\n",
      "|1      |452   |4.690034 |\n",
      "|1      |93    |4.5877824|\n",
      "|1      |12    |4.5567875|\n",
      "|1      |543   |4.5472484|\n",
      "|1      |99    |4.529876 |\n",
      "|1      |171   |4.518492 |\n",
      "|1      |169   |4.5059223|\n",
      "|2      |53    |4.718187 |\n",
      "|2      |43    |4.407033 |\n",
      "|2      |543   |4.353454 |\n",
      "|2      |276   |4.3002896|\n",
      "|2      |452   |4.239373 |\n",
      "|2      |12    |4.229362 |\n",
      "|2      |93    |4.2253966|\n",
      "|2      |584   |4.1333857|\n",
      "|2      |337   |4.111156 |\n",
      "|2      |578   |4.1066585|\n",
      "|3      |53    |4.446891 |\n",
      "|3      |43    |4.2145166|\n",
      "|3      |543   |3.9929395|\n",
      "|3      |276   |3.930692 |\n",
      "|3      |93    |3.9045453|\n",
      "|3      |452   |3.8745174|\n",
      "|3      |337   |3.854468 |\n",
      "|3      |171   |3.8544395|\n",
      "|3      |243   |3.7752533|\n",
      "|3      |12    |3.7746556|\n",
      "+-------+------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "best = tvs_model.bestModel\n",
    "TOPN = 10\n",
    "\n",
    "user_recs = best.recommendForAllUsers(TOPN)\n",
    "item_recs = best.recommendForAllItems(TOPN)\n",
    "\n",
    "# Pretty print a small sample\n",
    "user_recs.select(\"userId\", F.explode(\"recommendations\").alias(\"rec\")) \\\n",
    "    .select(\"userId\", F.col(\"rec.movieId\").alias(\"movieId\"), F.col(\"rec.rating\").alias(\"score\")) \\\n",
    "    .orderBy(\"userId\", F.desc(\"score\")).show(30, truncate=False)\n",
    "\n",
    "item_recs.select(\"movieId\", F.explode(\"recommendations\").alias(\"rec\")) \\\n",
    "    .select(\"movieId\", F.col(\"rec.userId\").alias(\"userId\"), F.col(\"rec.rating\").alias(\"score\")) \\\n",
    "    .orderBy(\"movieId\", F.desc(\"score\")).show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535efac4",
   "metadata": {},
   "source": [
    "## 8) Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a29dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 23:25:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/10/28 23:25:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/10/28 23:25:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /Users/alanyu/Documents/IIT/ITM/ITMD-524-Applied AI and Deep Learning/finalproject/MovieLens-MCRS/models/als_movielens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/10/28 23:25:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_f5812b623bb5, rank=64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = str((BASE_DIR / 'models' / 'als_movielens').resolve())\n",
    "\n",
    "# Save\n",
    "best.write().overwrite().save(MODEL_DIR)\n",
    "print(f\"Model saved to: {MODEL_DIR}\")\n",
    "\n",
    "# Load (if needed)\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "loaded = ALSModel.load(MODEL_DIR)\n",
    "loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d29a89",
   "metadata": {},
   "source": [
    "## 9) (Optional) Write validation predictions snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f440a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation predictions written to /Users/alanyu/Documents/IIT/ITM/ITMD-524-Applied AI and Deep Learning/finalproject/MovieLens-MCRS/outputs/als_val_predictions\n"
     ]
    }
   ],
   "source": [
    "preds = best.transform(val)\n",
    "output_dir = (BASE_DIR / 'outputs' / 'als_val_predictions').resolve()\n",
    "(preds\n",
    " .select('userId', 'movieId', 'rating', F.round('prediction', 3).alias('prediction'))\n",
    " .coalesce(1)\n",
    " .write.mode('overwrite')\n",
    " .option('header', True)\n",
    " .csv(str(output_dir)))\n",
    "print(f\"Validation predictions written to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914229d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### SVD (Surprise) vs ALS (Spark) quick mapping\n",
    "- **Goal**: both approximate user×item rating matrix; compare with RMSE/MAE.\n",
    "- **Optimization**: SVD uses SGD with biases; ALS alternates closed‑form solves (can enable `nonnegative=True`).\n",
    "- **Cold start**: set `coldStartStrategy=\"drop\"` to drop NaN predictions for unseen users/items in validation.\n",
    "- **Implicit feedback**: set `implicitPrefs=True` **and** change data to confidence‑weighted interactions; metrics differ.\n",
    "- **Time‑aware split**: for production‑like evaluation, split per‑user by timestamp rather than random split.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens-mcrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
